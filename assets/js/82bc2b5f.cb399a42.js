"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[633],{8453:(e,n,o)=>{o.d(n,{R:()=>a,x:()=>r});var t=o(6540);const i={},s=t.createContext(i);function a(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(s.Provider,{value:n},e.children)}},9385:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"chapter-06-conversational-robotics/conversational","title":"Chapter 6: Conversational Robotics","description":"Focus: The convergence of LLMs and Robotics","source":"@site/docs/chapter-06-conversational-robotics/conversational.md","sourceDirName":"chapter-06-conversational-robotics","slug":"/chapter-06-conversational-robotics/conversational","permalink":"/Q4-Hackathon/docs/chapter-06-conversational-robotics/conversational","draft":false,"unlisted":false,"editUrl":"https://github.com/rabia758/Q4-Hackathon/tree/main/frontend/docs/chapter-06-conversational-robotics/conversational.md","tags":[],"version":"current","frontMatter":{"id":"conversational","title":"Chapter 6: Conversational Robotics"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 5: Humanoid Robot Development","permalink":"/Q4-Hackathon/docs/chapter-05-humanoid-development/development"}}');var i=o(4848),s=o(8453);const a={id:"conversational",title:"Chapter 6: Conversational Robotics"},r="Chapter 6: Conversational Robotics (VLA)",l={},c=[{value:"Focus: The convergence of LLMs and Robotics",id:"focus-the-convergence-of-llms-and-robotics",level:2},{value:"6.1 Voice-to-Action (OpenAI Whisper)",id:"61-voice-to-action-openai-whisper",level:2},{value:"6.2 Cognitive Planning with LLMs",id:"62-cognitive-planning-with-llms",level:2},{value:"6.3 Multi-modal Interaction",id:"63-multi-modal-interaction",level:2},{value:"6.4 The Capstone Project: The Autonomous Humanoid",id:"64-the-capstone-project-the-autonomous-humanoid",level:2},{value:"Learning Outcomes:",id:"learning-outcomes",level:2}];function h(e){const n={h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-6-conversational-robotics-vla",children:"Chapter 6: Conversational Robotics (VLA)"})}),"\n",(0,i.jsx)(n.h2,{id:"focus-the-convergence-of-llms-and-robotics",children:"Focus: The convergence of LLMs and Robotics"}),"\n",(0,i.jsx)(n.p,{children:"This module explores how Large Language Models (LLMs) allow robots to understand and execute natural language instructions."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"61-voice-to-action-openai-whisper",children:"6.1 Voice-to-Action (OpenAI Whisper)"}),"\n",(0,i.jsx)(n.p,{children:"Converting spoken human language into text for the robot's brain to process."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Setting up the Whisper node in ROS 2."}),"\n",(0,i.jsx)(n.li,{children:"Handling noise and distant speech in real-world environments."}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"62-cognitive-planning-with-llms",children:"6.2 Cognitive Planning with LLMs"}),"\n",(0,i.jsx)(n.p,{children:'Using models like GPT-4 or local LLMs to translate vague commands ("I\'m thirsty") into a series of actionable steps.'}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:'Identify "thirsty" means finding a bottle.'}),"\n",(0,i.jsx)(n.li,{children:"Search for a water bottle in the environment."}),"\n",(0,i.jsx)(n.li,{children:"Plan a path to the bottle."}),"\n",(0,i.jsx)(n.li,{children:"Grasp the bottle and bring it to the user."}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"63-multi-modal-interaction",children:"6.3 Multi-modal Interaction"}),"\n",(0,i.jsx)(n.p,{children:"Integrating speech, gestures, and vision to create a more natural experience."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Understanding user pointing (deictic gestures)."}),"\n",(0,i.jsx)(n.li,{children:"Using facial expressions or body language to indicate robot status."}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"64-the-capstone-project-the-autonomous-humanoid",children:"6.4 The Capstone Project: The Autonomous Humanoid"}),"\n",(0,i.jsx)(n.p,{children:"Combining everything learned throughout the quarter."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"The Challenge:"})," A simulated robot receives a voice command, plans a path, navigates obstacles, identifies an object using computer vision, and manipulates it."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Criteria:"})," Success is measured by the fluidity of interaction and the reliability of task completion."]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Build a Voice-to-Action pipeline using Whisper."}),"\n",(0,i.jsx)(n.li,{children:"Use LLMs as high-level task planners for ROS 2."}),"\n",(0,i.jsx)(n.li,{children:"Deliver a complete, end-to-end humanoid robotics application."}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}}}]);